---
title: "Project - Group 9 - Report"
author: "REICHARDT Léo, BAEUMLIN Thomas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(kableExtra)
library(here)
library(tidyverse)
library(knitr)
library(keras)
library(cloudml)
library(stringr)

```



# Introduction

Most of us tried to buy a beer at the supermarket when they were 17. This probably did not have serious consequences, however, it is not always the case. Take for exemple, 14 years old girl who died after drinking soda mixed with vodka at their party (more details at this [link](https://abcnews.go.com/Health/14-year-dies-alcohol-poisoning-slumber-party/story?id=14065038)

This is how we came up with the idea to build a tool for supermarket sellers, barmans or any kind of jobs that sells things forbid to the under 18.
We were pleased to work with image recognition and interested to know if we would be able to build a model that recognizes the age of a person, which seemed very complex.  


# Research Question

Our model is itended to predict whether a person is over 18 years old or not by analyzing a facing picture of him.

# Previous analyse

For image recognition the classical aproach is to use CNN (convolutional neural networks)....

# Data

The data can be obtained from [kaggle](https://kaggle.com/frabbisw/facial-age). There are 99 folders named by the age of the person inside the folder. There are more than 9'OOO pictures in total. The images size is always the same (200x200 pixels) and contains three channels.

We have aggregated the pictures in two files. One containing the photos of people that are from 1 to 17 years old and another one with people over 18 years old.

Here is an example of our data:
```{r, fig.show='hold',fig.align='default',message=FALSE, echo=FALSE}

include_graphics(here::here("data/train/18-/5385.png"))
include_graphics(here::here("data/train/18+/5350.png"))


```

We couldn't find a best method to split the data into a training set and test set than changing the preview on our computer of the photos in 10 columns and taking 2 columns randomly to create the test set.


# Methodology


We started with a convolutional neural network using transfer learning (VGG16) and the feature extraction approach. The following hyperparameters are used : 
2 layers, 200 nodes, 0.00001 learning rate, no dropout.
It gave an accuracy of 0.916 and the job was completed in 15 minutes that will be our benchmark. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

x <- data.frame(ls_runs(runs_dir =  here::here("scripts/runs/")))
row.names(x) <- NULL
kable(x[ 23,c(1,4:7, 13)], caption = "<center><strong> Benchmark model </strong></center>",
    escape = FALSE,
    format = "html",
    row.names = FALSE) %>% kable_styling(bootstrap_options = c("striped", "bordered"))
```

We wanted to try another pre trained model: Inception_v3.
It is said to give great results when used for facial recognition. We used the exact same hyperparameters than before and got a poor 83% accuracy (in comparaison to VGG16) for a training that lasted 31 minutes. We abandoned this technique.

While looking how to use Inception_v3 preprocessing function, some forums said that VGG16, used with a preprocessing function, would yields better results than simply scaling the pixel values. We tried imagenet_preprocess_input.
The accuracy gave a 0.894 accuracy and took 36 minutes to complete the job which is much longer than using rescale function. We abandoned this technique too.


Then, we tried the other approach: fine-tuning. Using the same hyperparameters, it gave similar results but we decided to stick with this method as we were impress by how it performs with the flowers data. 


The next step was to test data augmentation. It worked remarkably! The accuracy jumped to 0.93. 
Note: As the validation set should not be augmented, we were no longer able to split the training data in the function image_data_generator. So we foolishly decided to use the test set as a validation one, not thinking that it would lead to overfitting our test set. The training being very long (we made the same error bellow doing the grid search) we didn't have time to rectify this error. In our defense, we see that the difference in accuracy is small. 


At this stage, we knew that we were going to use VGG16 with a fine-tuning approach and to augment the training set. 
We decided to launch a grid search of 125 combinations by using the following hyperparameters:   
  * nodes : 200, 300, 400, 500, 600  
  * learning rate :0.000004, 0.000008, 0.00002, 0.00006, 0.0001  
  * dropout : 0.1, 0.2, 0.3, 0.4, 0.5  

Every top dense layers have the same number of nodes and the same dropout rate. 
The same learning rate is applied when we compile the weights of the top dense layers and of the weights of the upper layers of the convolutional base joined with the top dense layers.  

We compared the features of the 10 best models and they had in common a very low learning rate generally :0.000004 or 0.000008. It was difficult to find patterns regarding the optimal dropout rate. Concerning the number of nodes, it seemed that 200, 300 and 400 would gives better results than a higher number. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}


y=x[c(12:22) ,c(1,4:10, 12)]

y=y %>% arrange(desc(metric_val_acc))

y %>% kable( caption = "<center><strong> The 10 best model of the grid search </strong></center>",
    escape = FALSE,
    format = "html",
    row.names = FALSE) %>% kable_styling(bootstrap_options = c("striped", "bordered"))

```


We tried to increase the number of layers to 3 using the knowledge acquired the previous run. However, we did not get better results as shown in the following table. 
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

z=x[c(2:11) ,c(1, 4:10, 12)]
z[c(2:11),1]=z[c(2:11),1] %>% str_remove("/Users/thomasbaeumlin/projg09/scripts/runs/")
z=z %>% arrange(desc(metric_val_acc))

z %>%
  kable( caption = "<center><strong> the 10 best model with 3 layers </strong></center>",
    escape = FALSE,
    format = "html",
    row.names = FALSE) %>% kable_styling(bootstrap_options = c("striped", "bordered"))


```

In the end, we selected the following model :  
  * fine-tune approach  
  * augmented training set  
  * model base VGG16  
  * 2 layers  
  * number of nodes of 200
  * relu activation function  
  * dropout rate of 0.5  
  * learning rate of 0.000004 
  

```{r best, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

a=x[1 ,c(1:10, 12)]
a[1,1]=x[1,1] %>% str_remove("/Users/thomasbaeumlin/projg09/scripts/runs/")
a %>%
  kable( caption = "<center><strong> The best model </strong></center>",
    escape = FALSE,
    format = "html",
    row.names = FALSE) %>% 
  kable_styling(bootstrap_options = c("striped", "bordered"))%>% 
  column_spec(1, width = "10em")

```
We can see in the table \@ref(tab:best) that the accuracy on the test set is around 90%, which is below from what we expected the validation training.

There are still some possibilities we didn't have time to explore such as using other activation functions, changing the batch size or adding an initializer.  


# Prediction

In order to test our model in real life conditions we try to make predictions with our own pictures.

Here are the picures we use:

```{r logo, fig.show='hold',fig.align='default', echo=FALSE}

myimages= list.files(here::here("selfie-image-pred/"), pattern = ".jpg", full.names = TRUE)

include_graphics(myimages)


```

Let's see the results:

```{r test,fig.show='hold',fig.align='default', echo=FALSE, warning=FALSE}

table_pred=read_csv(here::here("table_pred.csv"))

table_pred %>% select(Picture_of, Prob_Adult,Prob_Minor,Prob_Class)%>% kable(
    caption = "The prediction table",
    digits = 2
     ) %>%
  kable_styling(bootstrap_options = "striped")

```
As we can see in the table \@ref(tab:test) , the model have predict correctly the class for everyone. We can highlight the probablity to be Adult for Léo is around 60% we should probably ask for his ID card :).

# Conclusion 

We are satisfied with our results. Indeed, we have an accuracy of approximately 90%. Nevertheless, we are not sure about the "difficulty" of the dataset. In fact, the task is much harder to distinguish the age of people being 14 to 25 years old and we do not know the proportion of person it represents in the data.  
However, as a kind of final test, we added our own selfies to the project to check our tool and it predicted correctly that we are over 18. 

